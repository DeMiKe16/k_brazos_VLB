{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c012468d",
      "metadata": {},
      "source": [
        "# Introduccón del problema\n",
        "El **problema del bandido multi-brazo** (Multi-Armed Bandit) es un problema de toma de decisiones secuenciales en el que un agente debe seleccionar entre varias opciones en nuestro caso los llamados brazos, comunmente representados como brazos de maquinas tragaperras, para maximizar la recompensa acumulada a lo largo del tiempo. Cada opción tiene una probabilidad de  generar una recompensa, y el agente debe decidir entre **explorar** nuevas opciones para aprender más sobre ellas o **explotar** las opciones que ya parecen más rentables. Este dilema de equilibrar la exploración y la explotación es la clave del problema.\n",
        "\n",
        "El objetivo es aprender de manera eficiente cuál es la mejor opción sin desperdiciar demasiado tiempo en opciones subóptimas. Este problema se aplica en áreas como la optimización de anuncios en línea, la asignación de recursos y el diseño de sistemas de recomendación, donde las decisiones deben tomarse de forma continua y adaptativa."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45718ddbdacc17ac",
      "metadata": {
        "id": "45718ddbdacc17ac"
      },
      "source": [
        "# Experimento\n",
        "\n",
        "Se llevan a cabo varios experimentos para evaluar el rendimiento de diferentes algoritmos dentro de las familias **epsilon-greedy**, **UCB** y **Ascenso del Gradiente** en un problema de bandido multi-brazo con distribuciones **Normal**, **Bernoulli** y **Binomial**. No se realizan comparaciones entre algoritmos de diferentes familias ni se utilizan múltiples semillas.\n",
        "\n",
        "- Se generan gráficos que muestran la recompensa promedio obtenida por cada algoritmo.\n",
        "- Se generan gráficos que muestran la frecuencia de selecciones óptimas realizadas por cada algoritmo.\n",
        "- Se generan gráficos que detallan las estadísticas de los brazos seleccionados por cada algoritmo.\n",
        "- Se generan gráficos que representan el arrepentimiento promedio de cada algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d984c5",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd3d4a1b",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d5f97e0",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebb10323",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
