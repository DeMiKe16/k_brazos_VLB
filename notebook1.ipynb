{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeMiKe16/k_brazos_VLB/blob/main/notebook1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c012468d",
      "metadata": {
        "id": "c012468d"
      },
      "source": [
        "# Introduccón del problema\n",
        "El **problema del bandido multi-brazo** (Multi-Armed Bandit) es un problema de toma de decisiones secuenciales en el que un agente debe seleccionar entre varias opciones en nuestro caso los llamados brazos, comunmente representados como brazos de maquinas tragaperras, para maximizar la recompensa acumulada a lo largo del tiempo. Cada opción tiene una probabilidad de  generar una recompensa, y el agente debe decidir entre **explorar** nuevas opciones para aprender más sobre ellas o **explotar** las opciones que ya parecen más rentables. Este dilema de equilibrar la exploración y la explotación es la clave del problema.\n",
        "\n",
        "El objetivo es aprender de manera eficiente cuál es la mejor opción sin desperdiciar demasiado tiempo en opciones subóptimas. Este problema se aplica en áreas como la optimización de anuncios en línea, la asignación de recursos y el diseño de sistemas de recomendación, donde las decisiones deben tomarse de forma continua y adaptativa."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/DeMiKe16/k_brazos_VLB.git"
      ],
      "metadata": {
        "id": "QsSLagtAskCJ"
      },
      "id": "QsSLagtAskCJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "45718ddbdacc17ac",
      "metadata": {
        "id": "45718ddbdacc17ac"
      },
      "source": [
        "# Experimento\n",
        "\n",
        "Se llevan a cabo varios experimentos para evaluar el rendimiento de diferentes algoritmos dentro de las familias **epsilon-greedy**, **UCB** y **Ascenso del Gradiente** en un problema de bandido multi-brazo con distribuciones **Normal**, **Bernoulli** y **Binomial**. No se realizan comparaciones entre algoritmos de diferentes familias ni se utilizan múltiples semillas.\n",
        "\n",
        "- Se generan gráficos que muestran la recompensa promedio obtenida por cada algoritmo.\n",
        "- Se generan gráficos que muestran la frecuencia de selecciones óptimas realizadas por cada algoritmo.\n",
        "- Se generan gráficos que detallan las estadísticas de los brazos seleccionados por cada algoritmo.\n",
        "- Se generan gráficos que representan el arrepentimiento promedio de cada algoritmo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "939390cf",
      "metadata": {
        "id": "939390cf"
      },
      "source": [
        "# Epsilon-Greedy\n",
        "El algoritmo epsilon-greedy equilibra la exploración y la explotación seleccionando la mejor opción conocida con probabilidad 1 - ε y explorando aleatoriamente con probabilidad ε. Al principio, se asigna un valor alto a ε para fomentar la exploración, y con el tiempo se puede reducir para favorecer la explotación de la mejor opción encontrada.\n",
        "\n",
        "Este será un estudio del bandido multi-brazo en el que se usara el algoritmo de Epsilon-Greedy. Se realizarán 3 experimentos variando el porcentaje de exploración del algoritmo. Partiendo inicialmente de un algoritmo sin exploración, luego con una épsilon de un 1% de exploración y finalmente con una épsilon de un 10% de exploración. En cada experimento se trabajará con 3 bandidos que siguen cada uno distribuciones diferentes, empezando por Normal, siguiendo con Binomial y acabando con Bernoulli."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd497b33",
      "metadata": {
        "id": "fd497b33"
      },
      "source": [
        "# UCB\n",
        "UCB1 utiliza una estrategia basada en la incertidumbre, eligiendo el brazo con la mayor suma de su recompensa promedio y un término de exploración que decrece con el número de veces que un brazo ha sido seleccionado. Este enfoque permite explorar más al inicio y explotar conforme se reduce la incertidumbre.\n",
        "\n",
        "UCB2 es una variante de UCB1 que introduce una estrategia más controlada para la exploración, dividiendo el número de selecciones en intervalos de crecimiento logarítmico. Esto permite una mejor gestión entre exploración y explotación, reduciendo el arrepentimiento a largo plazo.\n",
        "\n",
        "Este será un estudio del bandido multi-brazo en el que se usaran los algoritmo UCB1 y UCB2, pertenecientes a la familia UCB y explicados anteriormente. Relizandose 2 experiementos y al igual que antes se usaran 3 bandidos por experimento siguiendo las distribuciones de probabilidad mencionadas anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43987f70",
      "metadata": {
        "id": "43987f70"
      },
      "source": [
        "#  Ascenso del Gradiente\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a4a0106",
      "metadata": {
        "id": "7a4a0106"
      },
      "source": [
        "El algoritmo Softmax asigna probabilidades a cada brazo en función de su recompensa esperada, usando una función exponencial controlada por un parámetro de temperatura (τ). Con valores altos de τ, la selección es más aleatoria (mayor exploración), mientras que con valores bajos se eligen con mayor probabilidad los brazos con mejores recompensas.\n",
        "\n",
        "Este enfoque usa gradientes de preferencia para actualizar la probabilidad de seleccionar cada brazo, basándose en la recompensa obtenida y comparándola con la recompensa media. Ajusta las probabilidades de selección mediante actualización estocástica, favoreciendo las acciones que han dado mejores resultados en el pasado.\n",
        "\n",
        "Este será un estudio del bandido multi-brazo en el que se usaran los algoritmo Softmax y Gradiente de Preferencias, explicados previamente. Relizandose 2 experiementos y al igual que antes se usaran 3 bandidos por experimento siguiendo las distribuciones de probabilidad mencionadas anteriormente."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}